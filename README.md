# Algorithms - COMSC 340 Project 2

### Introduction
The purpose of this project is to be able to look at the similarities between the theoretic analysis of an algorithm and the same algorithm on a real machine. The main objective is to understand how the theoretical analysis translates to actual performance, taking into account the impact of physical data structures and various performance measurement methods. 
To accomplish this, we are implementing three sorting algorithms: Insertion Sort using arrays, Insertion Sort using linked lists, and Merge Sort. We then run these algorithms on a series of input files containing varying numbers of elements in different orders (reverse, random, and ascending/sequential) across three different computers. This allows us to measure the run time of each algorithm on various hardware configurations. Furthermore, we add code to count basic operations, such as assignment, comparison, and arithmetic operations, to understand the underlying steps taken by each algorithm during the sorting process. By comparing the run time and the count of basic operations, we gain insights into the efficiency of each algorithm and its behavior on real machines.
In addition, we assembled our findings into a report that details our implementation process, testing methods, and performance analysis. The ultimate goal is to bridge the gap between theoretical algorithm analysis and real-world applications, highlighting the importance of understanding how algorithms perform in different contexts and the factors that can influence their efficiency.

### Methods
We opted for Python 3 as the programming language to implement the algorithms and data structures for this project, due to its ease of use and versatility. Our primary resources for the implementation were the pseudocode from our textbook and some examples from COMSC 111. During the implementation process, we paid special attention to counting basic operations, including assignment, comparison, addition, subtraction, multiplication, and division (excluding counters or indexers). We wanted to ensure that our analysis accurately accounted for these fundamental steps.
Before running the algorithms on larger input files, we tested them using a short, 10-element random-order text file. This testing phase allowed us to verify that our algorithms were functioning correctly and efficiently. Regarding our linked list implementation, we initially had to revise our first version based on Dr. Cates' advice. She recommended that we model our implementation after the one used in the COMSC 111 lab. Consequently, we made the necessary adjustments and ensured that our linked list implementation aligned with her suggestions.
In order to streamline the execution of our code and the analysis of its performance, we automated the file reading, sorting, and timing processes. We made a list containing all the file names and iterated through it using a loop. For each file, a custom function parsed the data, sorted it, and timed the sorting process. This automation not only saved us time and manual effort but also enabled us to efficiently run the code on all the input files. We employed the same method for counting basic operations for each algorithm on every input file.
To gather more comprehensive data on the performance of the algorithms across different hardware configurations, we executed our code on three distinct computers, each belonging to a team member. Lauren's computer was a Windows with an Intel processor; Hirwa's computer was a Macbook Pro with an Intel processor; and Louis's computer was a Macbook Air with the new M1 processor. By running our code on these diverse systems, we were able to obtain a more accurate and nuanced understanding of the relationship between theoretical algorithm analysis and the real-world performance of these algorithms on various machines.
